{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 센서 융합 (Sensor Fusion)\n",
    "\n",
    "이 노트북에서는 자율주행을 위한 LiDAR와 카메라 센서 융합 기법에 대해 학습하고, 실제 데이터를 활용하여 멀티모달 센서 융합을 구현합니다.\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "- 센서 융합의 기본 개념과 필요성 이해\n",
    "- 초기/중기/후기 융합 방식 비교 및 이해\n",
    "- 데이터 레벨 융합과 특징 레벨 융합 구현\n",
    "- LiDAR와 카메라 데이터 동기화 방법 학습\n",
    "- RGB-D 데이터 생성 및 활용\n",
    "- 멀티모달 특징 추출 및 융합 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from PIL import Image\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# 프로젝트 루트 경로 추가\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# 유틸리티 모듈 임포트\n",
    "from utils.calibration import (\n",
    "    load_calib_data,\n",
    "    lidar_to_camera,\n",
    "    camera_to_image,\n",
    "    lidar_to_image,\n",
    "    get_fov_mask\n",
    ")\n",
    "\n",
    "# 시각화 설정\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 센서 융합의 기본 개념과 필요성\n",
    "\n",
    "자율주행 시스템에서는 다양한 센서를 활용하여 주변 환경을 인식합니다. 각 센서는 고유한 장단점을 가지고 있습니다:\n",
    "\n",
    "### 센서별 특징\n",
    "\n",
    "- **LiDAR**:\n",
    "  - 장점: 정확한 거리 측정, 3D 구조 파악, 날씨/조명 영향 적음\n",
    "  - 단점: 해상도 낮음, 색상 정보 없음, 비용 높음\n",
    "\n",
    "- **카메라**:\n",
    "  - 장점: 높은 해상도, 풍부한 색상 정보, 질감 인식, 저렴한 비용\n",
    "  - 단점: 깊이 정보 부족, 조명/날씨 영향 큼, 거리 측정 어려움\n",
    "\n",
    "- **레이더**:\n",
    "  - 장점: 날씨 영향 적음, 속도 측정 가능, 장거리 감지\n",
    "  - 단점: 낮은 해상도, 각도 정확도 낮음\n",
    "\n",
    "### 센서 융합의 필요성\n",
    "\n",
    "센서 융합은 다양한 센서의 장점을 결합하고 단점을 보완하여 환경 인식의 신뢰성과 정확성을 높이는 것을 목표로 합니다:\n",
    "\n",
    "- **보완성**: 서로 다른 센서의 부족한 부분을 보완\n",
    "- **중복성**: 여러 센서가 동일한 정보를 제공하여 신뢰성 향상\n",
    "- **비용 효율성**: 고가의 센서와 저가의 센서 조합으로 성능과 비용 최적화\n",
    "- **강인성**: 한 센서의 실패에도 시스템 작동 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 센서 융합 방식\n",
    "\n",
    "센서 융합은 크게 다음과 같은 방식으로 나눌 수 있습니다:\n",
    "\n",
    "### 융합 단계에 따른 분류\n",
    "\n",
    "- **초기 융합 (Early Fusion)**:\n",
    "  - 원시 데이터 수준에서 융합\n",
    "  - 예: LiDAR 포인트를 카메라 이미지에 투영하여 RGB-D 데이터 생성\n",
    "\n",
    "- **중기 융합 (Mid Fusion)**:\n",
    "  - 특징 추출 후 특징 레벨에서 융합\n",
    "  - 예: LiDAR와 카메라에서 각각 특징을 추출한 후 결합\n",
    "\n",
    "- **후기 융합 (Late Fusion)**:\n",
    "  - 각 센서에서 독립적으로 결정한 결과를 최종 단계에서 융합\n",
    "  - 예: LiDAR와 카메라에서 각각 객체 검출 수행 후 결과 통합\n",
    "\n",
    "### 구현 방식에 따른 분류\n",
    "\n",
    "- **데이터 레벨 융합**: 센서 데이터를 직접 결합\n",
    "- **특징 레벨 융합**: 각 센서에서 추출한 특징을 결합\n",
    "- **결정 레벨 융합**: 각 센서에서 내린 결정을 통합\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 센서 융합 방식 개념도 시각화\n",
    "def visualize_fusion_methods():\n",
    "    \"\"\"센서 융합 방식의 개념도를 시각화\"\"\"\n",
    "    \n",
    "    # 이미지 생성\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    # 초기 융합 (Early Fusion)\n",
    "    ax = axes[0]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 4)\n",
    "    \n",
    "    # 센서 박스\n",
    "    ax.add_patch(plt.Rectangle((1, 1), 1.5, 1, fc='skyblue', ec='black', label='LiDAR'))\n",
    "    ax.add_patch(plt.Rectangle((1, 2.5), 1.5, 1, fc='lightgreen', ec='black', label='Camera'))\n",
    "    \n",
    "    # 융합 흐름\n",
    "    ax.arrow(2.5, 1.5, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    ax.arrow(2.5, 3, 1, -1, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # 융합 박스\n",
    "    ax.add_patch(plt.Rectangle((3.5, 1), 1.5, 1, fc='yellow', ec='black'))\n",
    "    ax.text(4.25, 1.5, 'Fusion', ha='center')\n",
    "    \n",
    "    # 처리 흐름\n",
    "    ax.arrow(5, 1.5, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # 특징 추출\n",
    "    ax.add_patch(plt.Rectangle((6, 1), 1.5, 1, fc='orange', ec='black'))\n",
    "    ax.text(6.75, 1.5, 'Features', ha='center')\n",
    "    \n",
    "    # 결정 흐름\n",
    "    ax.arrow(7.5, 1.5, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # 결정\n",
    "    ax.add_patch(plt.Rectangle((8.5, 1), 1.5, 1, fc='red', ec='black'))\n",
    "    ax.text(9.25, 1.5, 'Decision', ha='center')\n",
    "    \n",
    "    ax.set_title('Early Fusion')\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # 중기 융합 (Mid Fusion)\n",
    "    ax = axes[1]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 4)\n",
    "    \n",
    "    # 센서 박스\n",
    "    ax.add_patch(plt.Rectangle((1, 1), 1.5, 1, fc='skyblue', ec='black', label='LiDAR'))\n",
    "    ax.add_patch(plt.Rectangle((1, 2.5), 1.5, 1, fc='lightgreen', ec='black', label='Camera'))\n",
    "    \n",
    "    # 처리 흐름\n",
    "    ax.arrow(2.5, 1.5, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    ax.arrow(2.5, 3, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # 특징 추출\n",
    "    ax.add_patch(plt.Rectangle((3.5, 1), 1.5, 1, fc='orange', ec='black'))\n",
    "    ax.text(4.25, 1.5, 'LiDAR\\nFeatures', ha='center')\n",
    "    ax.add_patch(plt.Rectangle((3.5, 2.5), 1.5, 1, fc='orange', ec='black'))\n",
    "    ax.text(4.25, 3, 'Camera\\nFeatures', ha='center')\n",
    "    \n",
    "    # 융합 흐름\n",
    "    ax.arrow(5, 1.5, 1, 0.5, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    ax.arrow(5, 3, 1, -1, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # 융합 박스\n",
    "    ax.add_patch(plt.Rectangle((6, 1.5), 1.5, 1, fc='yellow', ec='black'))\n",
    "    ax.text(6.75, 2, 'Fusion', ha='center')\n",
    "    \n",
    "    # 결정 흐름\n",
    "    ax.arrow(7.5, 2, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # 결정\n",
    "    ax.add_patch(plt.Rectangle((8.5, 1.5), 1.5, 1, fc='red', ec='black'))\n",
    "    ax.text(9.25, 2, 'Decision', ha='center')\n",
    "    \n",
    "    ax.set_title('Mid Fusion')\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # 후기 융합 (Late Fusion)\n",
    "    ax = axes[2]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 4)\n",
    "    \n",
    "    # 센서 박스\n",
    "    ax.add_patch(plt.Rectangle((1, 1), 1.5, 1, fc='skyblue', ec='black', label='LiDAR'))\n",
    "    ax.add_patch(plt.Rectangle((1, 2.5), 1.5, 1, fc='lightgreen', ec='black', label='Camera'))\n",
    "    \n",
    "    # 처리 흐름\n",
    "    ax.arrow(2.5, 1.5, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    ax.arrow(2.5, 3, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # 특징 추출\n",
    "    ax.add_patch(plt.Rectangle((3.5, 1), 1.5, 1, fc='orange', ec='black'))\n",
    "    ax.text(4.25, 1.5, 'LiDAR\\nFeatures', ha='center')\n",
    "    ax.add_patch(plt.Rectangle((3.5, 2.5), 1.5, 1, fc='orange', ec='black'))\n",
    "    ax.text(4.25, 3, 'Camera\\nFeatures', ha='center')\n",
    "    \n",
    "    # 결정 흐름\n",
    "    ax.arrow(5, 1.5, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    ax.arrow(5, 3, 1, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # 개별 결정\n",
    "    ax.add_patch(plt.Rectangle((6, 1), 1.5, 1, fc='pink', ec='black'))\n",
    "    ax.text(6.75, 1.5, 'LiDAR\\nDecision', ha='center')\n",
    "    ax.add_patch(plt.Rectangle((6, 2.5), 1.5, 1, fc='pink', ec='black'))\n",
    "    ax.text(6.75, 3, 'Camera\\nDecision', ha='center')\n",
    "    \n",
    "    # 융합 흐름\n",
    "    ax.arrow(7.5, 1.5, 1, 0.5, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    ax.arrow(7.5, 3, 1, -1, head_width=0.1, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # 융합된 결정\n",
    "    ax.add_patch(plt.Rectangle((8.5, 1.5), 1.5, 1, fc='red', ec='black'))\n",
    "    ax.text(9.25, 2, 'Fusion\\nDecision', ha='center')\n",
    "    \n",
    "    ax.set_title('Late Fusion')\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # 범례 추가\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='skyblue', ec='black', label='LiDAR'),\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='lightgreen', ec='black', label='Camera'),\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='orange', ec='black', label='Feature Extraction'),\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='yellow', ec='black', label='Fusion'),\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='pink', ec='black', label='Individual Decision'),\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='red', ec='black', label='Final Decision')\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.show()\n",
    "\n",
    "# 센서 융합 방식 시각화\n",
    "visualize_fusion_methods()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 로드\n",
    "\n",
    "KITTI 데이터셋에서 LiDAR 포인트 클라우드, 카메라 이미지, 캘리브레이션 파일을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_point_cloud(bin_path):\n",
    "    \"\"\"\n",
    "    KITTI bin 형식의 포인트 클라우드 파일 로드\n",
    "    \n",
    "    Args:\n",
    "        bin_path (str): bin 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: 포인트 클라우드 [N, 4] (x, y, z, intensity)\n",
    "    \"\"\"\n",
    "    # 이진 파일에서 4xN float 배열로 읽기\n",
    "    points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)\n",
    "    return points\n",
    "\n",
    "def load_image(img_path):\n",
    "    \"\"\"\n",
    "    이미지 파일 로드\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): 이미지 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: RGB 이미지\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR -> RGB\n",
    "    return img\n",
    "\n",
    "# 데이터 경로 설정\n",
    "data_dir = '/home/shbyun/KITTI/ObjectDetection/'\n",
    "point_cloud_dir = os.path.join(data_dir, 'data_object_velodyne/training', 'velodyne')\n",
    "image_dir = os.path.join(data_dir, 'data_object_image_2/training', 'image_2')\n",
    "calib_dir = os.path.join(data_dir, 'data_object_calib/training', 'calib')\n",
    "\n",
    "# 첫 번째 프레임 로드\n",
    "frame_id = '000000'\n",
    "point_cloud_path = os.path.join(point_cloud_dir, f'{frame_id}.bin')\n",
    "image_path = os.path.join(image_dir, f'{frame_id}.png')\n",
    "calib_path = os.path.join(calib_dir, f'{frame_id}.txt')\n",
    "\n",
    "print(point_cloud_path)\n",
    "print(image_path)\n",
    "print(calib_path)\n",
    "\n",
    "# 데이터 로드\n",
    "if os.path.exists(point_cloud_path) and os.path.exists(image_path) and os.path.exists(calib_path):\n",
    "    points = load_point_cloud(point_cloud_path)\n",
    "    image = load_image(image_path)\n",
    "    calib_data = load_calib_data(calib_path)\n",
    "    \n",
    "    print(f\"포인트 클라우드 로드 완료: {points.shape}\")\n",
    "    print(f\"이미지 로드 완료: {image.shape}\")\n",
    "    print(f\"캘리브레이션 데이터 로드 완료: {list(calib_data.keys())}\")\n",
    "else:\n",
    "    # 데이터가 없는 경우 예시 데이터 생성\n",
    "    print(\"경로에 KITTI 데이터가 없습니다. 예시 데이터를 생성합니다...\")\n",
    "    \n",
    "    # 예시 포인트 클라우드 생성\n",
    "    num_points = 10000\n",
    "    x = np.random.uniform(-20, 20, num_points)\n",
    "    y = np.random.uniform(-20, 20, num_points)\n",
    "    z = np.random.uniform(-2, 5, num_points)\n",
    "    intensity = np.random.uniform(0, 1, num_points)\n",
    "    points = np.column_stack((x, y, z, intensity))\n",
    "    \n",
    "    # 예시 이미지 생성 (검은 배경에 그리드)\n",
    "    image = np.zeros((375, 1242, 3), dtype=np.uint8)\n",
    "    for i in range(0, image.shape[0], 50):\n",
    "        cv2.line(image, (0, i), (image.shape[1], i), (50, 50, 50), 1)\n",
    "    for i in range(0, image.shape[1], 50):\n",
    "        cv2.line(image, (i, 0), (i, image.shape[0]), (50, 50, 50), 1)\n",
    "    \n",
    "    # 렌덤 컬러 사각형 추가\n",
    "    for _ in range(5):\n",
    "        x1 = np.random.randint(0, image.shape[1] - 100)\n",
    "        y1 = np.random.randint(0, image.shape[0] - 100)\n",
    "        w = np.random.randint(50, 150)\n",
    "        h = np.random.randint(50, 150)\n",
    "        color = np.random.randint(0, 255, 3).tolist()\n",
    "        cv2.rectangle(image, (x1, y1), (x1+w, y1+h), color, -1)\n",
    "    \n",
    "    # 예시 캘리브레이션 데이터 생성\n",
    "    calib_data = {\n",
    "        'P2': np.array([\n",
    "            [721.5377, 0.0, 609.5593, 44.85728],\n",
    "            [0.0, 721.5377, 172.854, 0.2163791],\n",
    "            [0.0, 0.0, 1.0, 0.002745884]\n",
    "        ]),\n",
    "        'R0_rect': np.eye(3),\n",
    "        'Tr_velo_to_cam': np.array([\n",
    "            [7.533745e-03, -9.999714e-01, -6.166020e-04, -4.069766e-03],\n",
    "            [1.480249e-02, 7.280733e-04, -9.998902e-01, -7.631618e-02],\n",
    "            [9.998621e-01, 7.523790e-03, 1.480755e-02, -2.717806e-01],\n",
    "            [0.0, 0.0, 0.0, 1.0]\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    print(f\"예시 포인트 클라우드 생성 완료: {points.shape}\")\n",
    "    print(f\"예시 이미지 생성 완료: {image.shape}\")\n",
    "    print(f\"예시 캘리브레이션 데이터 생성 완료\")\n",
    "\n",
    "# 데이터 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(image)\n",
    "plt.title('Camera Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 포인트 클라우드 통계\n",
    "print(f\"포인트 개수: {points.shape[0]}\")\n",
    "print(f\"X 범위: [{np.min(points[:, 0]):.2f}, {np.max(points[:, 0]):.2f}]\")\n",
    "print(f\"Y 범위: [{np.min(points[:, 1]):.2f}, {np.max(points[:, 1]):.2f}]\")\n",
    "print(f\"Z 범위: [{np.min(points[:, 2]):.2f}, {np.max(points[:, 2]):.2f}]\")\n",
    "print(f\"강도 범위: [{np.min(points[:, 3]):.2f}, {np.max(points[:, 3]):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 초기 융합 (Early Fusion) - RGB-D 데이터 생성\n",
    "\n",
    "LiDAR 포인트 클라우드와 카메라 이미지를 조합하여 RGB-D 데이터를 생성합니다. 각 픽셀에 색상 정보(RGB)와 깊이 정보(D)를 모두 갖는 형태입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rgbd_data(points, image, calib_data, max_depth=100.0):\n",
    "    \"\"\"\n",
    "    LiDAR 포인트 클라우드와 카메라 이미지를 융합하여 RGB-D 데이터 생성\n",
    "    \n",
    "    Args:\n",
    "        points (numpy.ndarray): LiDAR 포인트 클라우드 [N, 4] (x, y, z, intensity)\n",
    "        image (numpy.ndarray): RGB 이미지\n",
    "        calib_data (dict): 캘리브레이션 데이터\n",
    "        max_depth (float): 최대 깊이 값\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (RGB-D 이미지, 깊이 이미지)\n",
    "    \"\"\"\n",
    "    # 이미지 크기 가져오기\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # 깊이 이미지 초기화 (모든 값 무한대로 설정)\n",
    "    depth_image = np.full((height, width), np.inf, dtype=np.float32)\n",
    "    \n",
    "    # LiDAR에서 이미지로 투영\n",
    "    points_img, points_cam = lidar_to_image(points, calib_data)\n",
    "    \n",
    "    # 이미지 범위 내 포인트 필터링\n",
    "    fov_mask = get_fov_mask(points_img, points_cam, (height, width))\n",
    "    points_img_fov = points_img[fov_mask].astype(int)\n",
    "    points_cam_fov = points_cam[fov_mask]\n",
    "    \n",
    "    # 깊이 값 (Z) 추출\n",
    "    depths = points_cam_fov[:, 2]\n",
    "    \n",
    "    # 깊이 이미지 생성 (각 픽셀에 대해 가장 가까운 포인트의 깊이 사용)\n",
    "    for i, (u, v) in enumerate(points_img_fov):\n",
    "        if 0 <= u < width and 0 <= v < height:\n",
    "            # 현재 저장된 깊이보다 더 가까우면 업데이트\n",
    "            if depths[i] < depth_image[v, u]:\n",
    "                depth_image[v, u] = depths[i]\n",
    "    \n",
    "    # RGB-D 이미지 생성 (RGB 채널 + 깊이 채널)\n",
    "    rgbd_image = np.dstack((image, depth_image))\n",
    "    \n",
    "    # 깊이 이미지 시각화를 위해 정규화\n",
    "    depth_viz = depth_image.copy()\n",
    "    mask = depth_viz != np.inf\n",
    "    if np.sum(mask) > 0:  # 유효한 깊이 값이 있는 경우\n",
    "        depth_viz[~mask] = 0  # 무한대 값을 0으로 설정\n",
    "        depth_viz[mask] = np.clip(depth_viz[mask], 0, max_depth)  # 최대 깊이 제한\n",
    "        depth_viz = depth_viz / max_depth * 255  # 0-255 범위로 정규화\n",
    "    \n",
    "    depth_viz = depth_viz.astype(np.uint8)\n",
    "    \n",
    "    return rgbd_image, depth_viz\n",
    "\n",
    "# RGB-D 데이터 생성\n",
    "rgbd_image, depth_viz = generate_rgbd_data(points, image, calib_data)\n",
    "\n",
    "# 결과 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RGB 이미지\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('RGB Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# 깊이 이미지\n",
    "depth_colored = cv2.applyColorMap(depth_viz, cv2.COLORMAP_PLASMA)\n",
    "depth_colored = cv2.cvtColor(depth_colored, cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(depth_colored)\n",
    "axes[1].set_title('Depth Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RGB-D 이미지에서 특징 추출\n",
    "\n",
    "RGB-D 이미지에서 색상 정보와 깊이 정보를 모두 활용하여 특징을 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rgbd_features(rgb_image, depth_image, block_size=16):\n",
    "    \"\"\"\n",
    "    RGB-D 이미지에서 색상과 깊이 특징 추출\n",
    "    \n",
    "    Args:\n",
    "        rgb_image (numpy.ndarray): RGB 이미지\n",
    "        depth_image (numpy.ndarray): 깊이 이미지\n",
    "        block_size (int): 특징 추출을 위한 블록 크기\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: 추출된 특징 벡터 [N, D]\n",
    "    \"\"\"\n",
    "    height, width = rgb_image.shape[:2]\n",
    "    \n",
    "    # 결과 특징 벡터를 저장할 리스트\n",
    "    features = []\n",
    "    \n",
    "    # 이미지를 블록으로 분할하여 처리\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            # 블록 경계 설정\n",
    "            y_end = min(y + block_size, height)\n",
    "            x_end = min(x + block_size, width)\n",
    "            \n",
    "            # 블록 추출\n",
    "            rgb_block = rgb_image[y:y_end, x:x_end]\n",
    "            depth_block = depth_image[y:y_end, x:x_end]\n",
    "            \n",
    "            # 유효한 깊이 값 마스크 (무한대가 아닌 값)\n",
    "            depth_mask = depth_block != np.inf\n",
    "            \n",
    "            # RGB 특징 (평균 색상)\n",
    "            rgb_mean = np.mean(rgb_block, axis=(0, 1))\n",
    "            \n",
    "            # 깊이 특징\n",
    "            if np.sum(depth_mask) > 0:\n",
    "                depth_mean = np.mean(depth_block[depth_mask])\n",
    "                depth_std = np.std(depth_block[depth_mask])\n",
    "            else:\n",
    "                depth_mean = 0\n",
    "                depth_std = 0\n",
    "            \n",
    "            # RGB와 깊이 특징 결합\n",
    "            block_feature = np.concatenate([rgb_mean, [depth_mean, depth_std]])\n",
    "            \n",
    "            # 중심 좌표 추가\n",
    "            center_x = (x + x_end) / 2 / width\n",
    "            center_y = (y + y_end) / 2 / height\n",
    "            \n",
    "            # 최종 특징 벡터\n",
    "            feature = np.concatenate([block_feature, [center_x, center_y]])\n",
    "            \n",
    "            features.append(feature)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# RGB 이미지와 깊이 이미지에서 특징 추출\n",
    "rgb_features = extract_rgbd_features(image, rgbd_image[:, :, 3], block_size=32)\n",
    "\n",
    "print(f\"추출된 특징 개수: {len(rgb_features)}\")\n",
    "print(f\"특징 벡터 차원: {rgb_features.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 중기 융합 (Mid Fusion) - 특징 융합\n",
    "\n",
    "LiDAR와 카메라에서 각각 추출한 특징을 융합하여 통합된 특징 벡터를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lidar_features(points, calib_data, image_shape, grid_size=5.0, height_bins=10):\n",
    "    \"\"\"\n",
    "    LiDAR 포인트 클라우드에서 특징 추출\n",
    "    \n",
    "    Args:\n",
    "        points (numpy.ndarray): LiDAR 포인트 클라우드 [N, 4] (x, y, z, intensity)\n",
    "        calib_data (dict): 캘리브레이션 데이터\n",
    "        image_shape (tuple): 이미지 크기 (height, width)\n",
    "        grid_size (float): 그리드 셀 크기 (미터)\n",
    "        height_bins (int): 높이 히스토그램 빈 개수\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: 추출된 LiDAR 특징 [M, D]\n",
    "    \"\"\"\n",
    "    # 이미지 평면으로 투영\n",
    "    points_img, points_cam = lidar_to_image(points, calib_data)\n",
    "    \n",
    "    # 이미지 범위 내 포인트 필터링\n",
    "    fov_mask = get_fov_mask(points_img, points_cam, image_shape)\n",
    "    points_fov = points[fov_mask]\n",
    "    points_cam_fov = points_cam[fov_mask]\n",
    "    \n",
    "    # 바닥면(XY) 그리드 생성\n",
    "    x_min, x_max = np.min(points_fov[:, 0]), np.max(points_fov[:, 0])\n",
    "    y_min, y_max = np.min(points_fov[:, 1]), np.max(points_fov[:, 1])\n",
    "    \n",
    "    # 그리드 인덱스 계산을 위한 경계 조정\n",
    "    x_min = np.floor(x_min / grid_size) * grid_size\n",
    "    y_min = np.floor(y_min / grid_size) * grid_size\n",
    "    x_max = np.ceil(x_max / grid_size) * grid_size\n",
    "    y_max = np.ceil(y_max / grid_size) * grid_size\n",
    "    \n",
    "    # 그리드 차원 계산\n",
    "    num_x_cells = int((x_max - x_min) / grid_size)\n",
    "    num_y_cells = int((y_max - y_min) / grid_size)\n",
    "    \n",
    "    # 그리드가 너무 작으면 조정\n",
    "    if num_x_cells < 2:\n",
    "        num_x_cells = 2\n",
    "        x_max = x_min + grid_size * num_x_cells\n",
    "    if num_y_cells < 2:\n",
    "        num_y_cells = 2\n",
    "        y_max = y_min + grid_size * num_y_cells\n",
    "    \n",
    "    # 높이 범위 계산\n",
    "    z_min, z_max = np.min(points_fov[:, 2]), np.max(points_fov[:, 2])\n",
    "    \n",
    "    # 특징 저장을 위한 리스트\n",
    "    lidar_features = []\n",
    "    \n",
    "    # 각 그리드 셀에 대해 특징 추출\n",
    "    for i in range(num_x_cells):\n",
    "        for j in range(num_y_cells):\n",
    "            # 그리드 셀 경계\n",
    "            cell_x_min = x_min + i * grid_size\n",
    "            cell_x_max = x_min + (i + 1) * grid_size\n",
    "            cell_y_min = y_min + j * grid_size\n",
    "            cell_y_max = y_min + (j + 1) * grid_size\n",
    "            \n",
    "            # 셀 내 포인트 필터링\n",
    "            cell_mask = (\n",
    "                (points_fov[:, 0] >= cell_x_min) & (points_fov[:, 0] < cell_x_max) &\n",
    "                (points_fov[:, 1] >= cell_y_min) & (points_fov[:, 1] < cell_y_max)\n",
    "            )\n",
    "            cell_points = points_fov[cell_mask]\n",
    "            \n",
    "            if len(cell_points) > 0:\n",
    "                # 기본 통계 특징\n",
    "                mean_z = np.mean(cell_points[:, 2])\n",
    "                std_z = np.std(cell_points[:, 2])\n",
    "                mean_intensity = np.mean(cell_points[:, 3])\n",
    "                std_intensity = np.std(cell_points[:, 3])\n",
    "                point_density = len(cell_points) / (grid_size * grid_size)\n",
    "                \n",
    "                # 높이 히스토그램\n",
    "                hist, _ = np.histogram(cell_points[:, 2], bins=height_bins, range=(z_min, z_max))\n",
    "                hist = hist / (np.sum(hist) + 1e-10)  # 정규화\n",
    "                \n",
    "                # 셀 중심 좌표 (정규화)\n",
    "                center_x = (cell_x_min + cell_x_max) / 2\n",
    "                center_y = (cell_y_min + cell_y_max) / 2\n",
    "                \n",
    "                # 특징 벡터 결합\n",
    "                feature = np.concatenate([\n",
    "                    [mean_z, std_z, mean_intensity, std_intensity, point_density, center_x, center_y],\n",
    "                    hist\n",
    "                ])\n",
    "                \n",
    "                lidar_features.append(feature)\n",
    "    \n",
    "    return np.array(lidar_features)\n",
    "\n",
    "# 중기 융합 모델 정의 (간단한 MLP)\n",
    "class MidFusionMLP(nn.Module):\n",
    "    def __init__(self, camera_dim, lidar_dim, hidden_dim=64, output_dim=32):\n",
    "        super(MidFusionMLP, self).__init__()\n",
    "        \n",
    "        # 카메라 브랜치\n",
    "        self.camera_branch = nn.Sequential(\n",
    "            nn.Linear(camera_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # LiDAR 브랜치\n",
    "        self.lidar_branch = nn.Sequential(\n",
    "            nn.Linear(lidar_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 융합 브랜치\n",
    "        self.fusion_branch = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, camera_features, lidar_features):\n",
    "        # 카메라 특징 처리\n",
    "        camera_out = self.camera_branch(camera_features)\n",
    "        \n",
    "        # LiDAR 특징 처리\n",
    "        lidar_out = self.lidar_branch(lidar_features)\n",
    "        \n",
    "        # 특징 융합 (단순 연결)\n",
    "        fused = torch.cat([camera_out, lidar_out], dim=1)\n",
    "        \n",
    "        # 융합된 특징 처리\n",
    "        output = self.fusion_branch(fused)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# LiDAR 특징 추출\n",
    "lidar_features = extract_lidar_features(points, calib_data, image.shape[:2], grid_size=5.0)\n",
    "\n",
    "print(f\"LiDAR 특징 개수: {len(lidar_features)}\")\n",
    "print(f\"LiDAR 특징 차원: {lidar_features.shape[1]}\")\n",
    "print(f\"카메라 특징 개수: {len(rgb_features)}\")\n",
    "print(f\"카메라 특징 차원: {rgb_features.shape[1]}\")\n",
    "\n",
    "# 시뮬레이션을 위해 동일한 수의 샘플만 사용\n",
    "min_samples = min(len(rgb_features), len(lidar_features))\n",
    "rgb_features_sample = rgb_features[:min_samples]\n",
    "lidar_features_sample = lidar_features[:min_samples]\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "camera_tensor = torch.FloatTensor(rgb_features_sample)\n",
    "lidar_tensor = torch.FloatTensor(lidar_features_sample)\n",
    "\n",
    "# 중기 융합 모델 초기화\n",
    "mid_fusion_model = MidFusionMLP(\n",
    "    camera_dim=rgb_features.shape[1],\n",
    "    lidar_dim=lidar_features.shape[1],\n",
    "    hidden_dim=64,\n",
    "    output_dim=32\n",
    ")\n",
    "\n",
    "# 모델 순전파 시연\n",
    "fused_features = mid_fusion_model(camera_tensor, lidar_tensor)\n",
    "\n",
    "print(f\"융합된 특징 텐서 크기: {fused_features.shape}\")\n",
    "print(f\"샘플 융합 특징: {fused_features[0][:5]}\")  # 첫 번째 샘플의 처음 5개 요소\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 후기 융합 (Late Fusion) - 결정 융합\n",
    "\n",
    "LiDAR와 카메라에서 각각 독립적으로 수행한 분류 또는 탐지 결과를 융합합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_object_detection(num_objects=10, img_width=1242, img_height=375, seed=42):\n",
    "    \"\"\"\n",
    "    객체 검출 결과 시뮬레이션\n",
    "    \n",
    "    Args:\n",
    "        num_objects (int): 생성할 객체 수\n",
    "        img_width (int): 이미지 너비\n",
    "        img_height (int): 이미지 높이\n",
    "        seed (int): 랜덤 시드\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (카메라 검출 결과, LiDAR 검출 결과)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 객체 클래스\n",
    "    classes = ['Car', 'Pedestrian', 'Cyclist']\n",
    "    \n",
    "    # 카메라 검출 결과 생성\n",
    "    camera_detections = []\n",
    "    for i in range(num_objects):\n",
    "        # 무작위 경계 상자\n",
    "        x1 = np.random.randint(0, img_width - 100)\n",
    "        y1 = np.random.randint(0, img_height - 100)\n",
    "        w = np.random.randint(50, 100)\n",
    "        h = np.random.randint(50, 100)\n",
    "        \n",
    "        # 무작위 클래스 및 신뢰도\n",
    "        class_id = np.random.randint(0, len(classes))\n",
    "        confidence = np.random.uniform(0.5, 1.0)\n",
    "        \n",
    "        # 각 객체에 고유 ID 부여\n",
    "        object_id = i\n",
    "        \n",
    "        camera_detections.append({\n",
    "            'box': [x1, y1, x1+w, y1+h],\n",
    "            'class': classes[class_id],\n",
    "            'confidence': confidence,\n",
    "            'id': object_id\n",
    "        })\n",
    "    \n",
    "    # LiDAR 검출 결과 생성 (일부 카메라 검출과 일치, 일부는 새로운 검출)\n",
    "    lidar_detections = []\n",
    "    \n",
    "    # 카메라 검출과 일치하는 일부 객체\n",
    "    for i in range(min(7, num_objects)):\n",
    "        camera_det = camera_detections[i].copy()\n",
    "        \n",
    "        # 약간의 변동 추가\n",
    "        box = camera_det['box']\n",
    "        box = [\n",
    "            max(0, box[0] + np.random.randint(-10, 10)),\n",
    "            max(0, box[1] + np.random.randint(-10, 10)),\n",
    "            min(img_width, box[2] + np.random.randint(-10, 10)),\n",
    "            min(img_height, box[3] + np.random.randint(-10, 10))\n",
    "        ]\n",
    "        \n",
    "        # 신뢰도 변경\n",
    "        confidence = np.random.uniform(0.5, 1.0)\n",
    "        \n",
    "        lidar_detections.append({\n",
    "            'box': box,\n",
    "            'class': camera_det['class'],\n",
    "            'confidence': confidence,\n",
    "            'id': camera_det['id']\n",
    "        })\n",
    "    \n",
    "    # 새로운 LiDAR 검출 추가\n",
    "    for i in range(3):\n",
    "        # 무작위 경계 상자\n",
    "        x1 = np.random.randint(0, img_width - 100)\n",
    "        y1 = np.random.randint(0, img_height - 100)\n",
    "        w = np.random.randint(50, 100)\n",
    "        h = np.random.randint(50, 100)\n",
    "        \n",
    "        # 무작위 클래스 및 신뢰도\n",
    "        class_id = np.random.randint(0, len(classes))\n",
    "        confidence = np.random.uniform(0.5, 1.0)\n",
    "        \n",
    "        # 새로운 ID 부여\n",
    "        object_id = num_objects + i\n",
    "        \n",
    "        lidar_detections.append({\n",
    "            'box': [x1, y1, x1+w, y1+h],\n",
    "            'class': classes[class_id],\n",
    "            'confidence': confidence,\n",
    "            'id': object_id\n",
    "        })\n",
    "    \n",
    "    return camera_detections, lidar_detections\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    두 경계 상자 간의 IoU(Intersection over Union) 계산\n",
    "    \n",
    "    Args:\n",
    "        box1 (list): 첫 번째 경계 상자 [x1, y1, x2, y2]\n",
    "        box2 (list): 두 번째 경계 상자 [x1, y1, x2, y2]\n",
    "        \n",
    "    Returns:\n",
    "        float: IoU 값\n",
    "    \"\"\"\n",
    "    # 교차 영역 계산\n",
    "    x_left = max(box1[0], box2[0])\n",
    "    y_top = max(box1[1], box2[1])\n",
    "    x_right = min(box1[2], box2[2])\n",
    "    y_bottom = min(box1[3], box2[3])\n",
    "    \n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "    # 각 경계 상자 영역 계산\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # IoU 계산\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def late_fusion(camera_detections, lidar_detections, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    카메라와 LiDAR 검출 결과를 융합 (후기 융합)\n",
    "    \n",
    "    Args:\n",
    "        camera_detections (list): 카메라 검출 결과 리스트\n",
    "        lidar_detections (list): LiDAR 검출 결과 리스트\n",
    "        iou_threshold (float): 동일 객체로 판단하는 IoU 임계값\n",
    "        \n",
    "    Returns:\n",
    "        list: 융합된 검출 결과\n",
    "    \"\"\"\n",
    "    fused_detections = []\n",
    "    used_camera_indices = set()\n",
    "    used_lidar_indices = set()\n",
    "    \n",
    "    # 각 카메라 검출에 대해 가장 일치하는 LiDAR 검출 찾기\n",
    "    for i, camera_det in enumerate(camera_detections):\n",
    "        camera_box = camera_det['box']\n",
    "        best_iou = 0\n",
    "        best_match = -1\n",
    "        \n",
    "        for j, lidar_det in enumerate(lidar_detections):\n",
    "            lidar_box = lidar_det['box']\n",
    "            iou = compute_iou(camera_box, lidar_box)\n",
    "            \n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match = j\n",
    "        \n",
    "        # 충분히 일치하는 LiDAR 검출이 있는 경우 융합\n",
    "        if best_iou > iou_threshold:\n",
    "            lidar_det = lidar_detections[best_match]\n",
    "            \n",
    "            # 신뢰도 가중 평균 (일반적으로 각 센서의 특성에 따라 가중치를 다르게 설정)\n",
    "            fused_confidence = 0.5 * camera_det['confidence'] + 0.5 * lidar_det['confidence']\n",
    "            \n",
    "            # 클래스가 다른 경우 더 높은 신뢰도의 클래스 선택\n",
    "            if camera_det['class'] != lidar_det['class']:\n",
    "                fused_class = camera_det['class'] if camera_det['confidence'] > lidar_det['confidence'] else lidar_det['class']\n",
    "            else:\n",
    "                fused_class = camera_det['class']\n",
    "            \n",
    "            # IoU 기반으로 박스 위치 융합\n",
    "            fused_box = [\n",
    "                (camera_det['box'][0] + lidar_det['box'][0]) / 2,\n",
    "                (camera_det['box'][1] + lidar_det['box'][1]) / 2,\n",
    "                (camera_det['box'][2] + lidar_det['box'][2]) / 2,\n",
    "                (camera_det['box'][3] + lidar_det['box'][3]) / 2\n",
    "            ]\n",
    "            \n",
    "            fused_detections.append({\n",
    "                'box': fused_box,\n",
    "                'class': fused_class,\n",
    "                'confidence': fused_confidence,\n",
    "                'id': camera_det['id'],\n",
    "                'source': 'both',\n",
    "                'camera_confidence': camera_det['confidence'],\n",
    "                'lidar_confidence': lidar_det['confidence']\n",
    "            })\n",
    "            \n",
    "            used_camera_indices.add(i)\n",
    "            used_lidar_indices.add(best_match)\n",
    "        \n",
    "    # 매칭되지 않은 카메라 검출 추가\n",
    "    for i, camera_det in enumerate(camera_detections):\n",
    "        if i not in used_camera_indices:\n",
    "            camera_det_copy = camera_det.copy()\n",
    "            camera_det_copy['source'] = 'camera'\n",
    "            camera_det_copy['camera_confidence'] = camera_det['confidence']\n",
    "            camera_det_copy['lidar_confidence'] = 0.0\n",
    "            fused_detections.append(camera_det_copy)\n",
    "    \n",
    "    # 매칭되지 않은 LiDAR 검출 추가\n",
    "    for i, lidar_det in enumerate(lidar_detections):\n",
    "        if i not in used_lidar_indices:\n",
    "            lidar_det_copy = lidar_det.copy()\n",
    "            lidar_det_copy['source'] = 'lidar'\n",
    "            lidar_det_copy['camera_confidence'] = 0.0\n",
    "            lidar_det_copy['lidar_confidence'] = lidar_det['confidence']\n",
    "            fused_detections.append(lidar_det_copy)\n",
    "    \n",
    "    return fused_detections\n",
    "\n",
    "# 객체 검출 결과 시뮬레이션\n",
    "camera_detections, lidar_detections = simulate_object_detection(num_objects=8)\n",
    "\n",
    "# 후기 융합 적용\n",
    "fused_detections = late_fusion(camera_detections, lidar_detections, iou_threshold=0.3)\n",
    "\n",
    "print(f\"카메라 검출 수: {len(camera_detections)}\")\n",
    "print(f\"LiDAR 검출 수: {len(lidar_detections)}\")\n",
    "print(f\"융합 검출 수: {len(fused_detections)}\")\n",
    "\n",
    "# 검출 결과 시각화\n",
    "def visualize_detections(image, camera_detections, lidar_detections, fused_detections):\n",
    "    \"\"\"검출 결과 시각화\"\"\"\n",
    "    # 이미지 복사\n",
    "    camera_img = image.copy()\n",
    "    lidar_img = image.copy()\n",
    "    fused_img = image.copy()\n",
    "    \n",
    "    # 카메라 검출 시각화\n",
    "    for det in camera_detections:\n",
    "        box = det['box']\n",
    "        box = [int(x) for x in box]\n",
    "        cv2.rectangle(camera_img, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        label = f\"{det['class']} {det['confidence']:.2f}\"\n",
    "        cv2.putText(camera_img, label, (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # LiDAR 검출 시각화\n",
    "    for det in lidar_detections:\n",
    "        box = det['box']\n",
    "        box = [int(x) for x in box]\n",
    "        cv2.rectangle(lidar_img, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
    "        label = f\"{det['class']} {det['confidence']:.2f}\"\n",
    "        cv2.putText(lidar_img, label, (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    # 융합 검출 시각화\n",
    "    for det in fused_detections:\n",
    "        box = det['box']\n",
    "        box = [int(x) for x in box]\n",
    "        \n",
    "        # 소스에 따라 색상 설정\n",
    "        if det['source'] == 'both':\n",
    "            color = (255, 0, 255)  # 마젠타\n",
    "        elif det['source'] == 'camera':\n",
    "            color = (0, 255, 0)    # 초록\n",
    "        else:  # 'lidar'\n",
    "            color = (255, 0, 0)    # 빨강\n",
    "            \n",
    "        cv2.rectangle(fused_img, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "        label = f\"{det['class']} {det['confidence']:.2f}\"\n",
    "        cv2.putText(fused_img, label, (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    # 결과 표시\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(camera_img)\n",
    "    axes[0].set_title('Camera Detections')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(lidar_img)\n",
    "    axes[1].set_title('LiDAR Detections')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(fused_img)\n",
    "    axes[2].set_title('Fused Detections')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 검출 결과 시각화\n",
    "visualize_detections(image, camera_detections, lidar_detections, fused_detections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 고급: 3D-2D 융합 파이프라인\n",
    "\n",
    "LiDAR 기반 3D 객체 검출과 카메라 기반 2D 객체 검출을 융합하는 파이프라인을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_3d_detections(num_objects=5, seed=42):\n",
    "    \"\"\"\n",
    "    3D 객체 검출 결과 시뮬레이션\n",
    "    \n",
    "    Args:\n",
    "        num_objects (int): 생성할 객체 수\n",
    "        seed (int): 랜덤 시드\n",
    "        \n",
    "    Returns:\n",
    "        list: 3D 객체 검출 결과\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 객체 클래스\n",
    "    classes = ['Car', 'Pedestrian', 'Cyclist']\n",
    "    \n",
    "    # 3D 객체 검출 결과 생성\n",
    "    detections_3d = []\n",
    "    \n",
    "    for i in range(num_objects):\n",
    "        # 무작위 3D 위치 (x, y, z)\n",
    "        x = np.random.uniform(-10, 10)\n",
    "        y = np.random.uniform(5, 30)\n",
    "        z = np.random.uniform(-1, 1)\n",
    "        \n",
    "        # 무작위 3D 크기 (length, width, height)\n",
    "        if np.random.rand() < 0.7:  # Car\n",
    "            length = np.random.uniform(3.5, 5.0)\n",
    "            width = np.random.uniform(1.5, 2.0)\n",
    "            height = np.random.uniform(1.4, 1.8)\n",
    "            class_name = 'Car'\n",
    "        elif np.random.rand() < 0.5:  # Pedestrian\n",
    "            length = np.random.uniform(0.4, 0.8)\n",
    "            width = np.random.uniform(0.4, 0.8)\n",
    "            height = np.random.uniform(1.4, 1.9)\n",
    "            class_name = 'Pedestrian'\n",
    "        else:  # Cyclist\n",
    "            length = np.random.uniform(1.0, 1.8)\n",
    "            width = np.random.uniform(0.5, 0.8)\n",
    "            height = np.random.uniform(1.4, 1.9)\n",
    "            class_name = 'Cyclist'\n",
    "        \n",
    "        # 무작위 방향 (yaw 각도)\n",
    "        yaw = np.random.uniform(-np.pi, np.pi)\n",
    "        \n",
    "        # 신뢰도\n",
    "        confidence = np.random.uniform(0.5, 1.0)\n",
    "        \n",
    "        detections_3d.append({\n",
    "            'id': i,\n",
    "            'position': np.array([x, y, z]),\n",
    "            'dimensions': np.array([length, width, height]),\n",
    "            'yaw': yaw,\n",
    "            'class': class_name,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    return detections_3d\n",
    "\n",
    "def project_3d_to_2d(detections_3d, calib_data, image_shape):\n",
    "    \"\"\"\n",
    "    3D 검출 결과를 이미지 평면에 투영\n",
    "    \n",
    "    Args:\n",
    "        detections_3d (list): 3D 객체 검출 결과\n",
    "        calib_data (dict): 캘리브레이션 데이터\n",
    "        image_shape (tuple): 이미지 크기 (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        list: 2D 경계 상자가 추가된 검출 결과\n",
    "    \"\"\"\n",
    "    height, width = image_shape\n",
    "    results = []\n",
    "    \n",
    "    for det in detections_3d:\n",
    "        # 객체 중심점과 크기\n",
    "        pos = det['position']\n",
    "        dims = det['dimensions']\n",
    "        yaw = det['yaw']\n",
    "        \n",
    "        # 3D 바운딩 박스의 8개 코너 계산\n",
    "        # 바운딩 박스는 차량 좌표계에서 정의됨 (z-up, x-forward, y-left)\n",
    "        l, w, h = dims\n",
    "        \n",
    "        # 바운딩 박스 코너 (차량 좌표계)\n",
    "        corners_3d = np.array([\n",
    "            [l/2, l/2, -l/2, -l/2, l/2, l/2, -l/2, -l/2],\n",
    "            [w/2, -w/2, -w/2, w/2, w/2, -w/2, -w/2, w/2],\n",
    "            [0, 0, 0, 0, h, h, h, h]\n",
    "        ])\n",
    "        \n",
    "        # 회전 변환 (yaw 회전)\n",
    "        R = np.array([\n",
    "            [np.cos(yaw), -np.sin(yaw), 0],\n",
    "            [np.sin(yaw), np.cos(yaw), 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        # 회전 적용\n",
    "        corners_3d = R @ corners_3d\n",
    "        \n",
    "        # 위치 이동\n",
    "        corners_3d[0, :] += pos[0]\n",
    "        corners_3d[1, :] += pos[1]\n",
    "        corners_3d[2, :] += pos[2]\n",
    "        \n",
    "        # LiDAR 좌표계에서 카메라 좌표계로 변환\n",
    "        corners_cam = np.zeros_like(corners_3d)\n",
    "        for i in range(8):\n",
    "            point_lidar = np.append(corners_3d[:, i], 1)\n",
    "            point_cam = calib_data['Tr_velo_to_cam'] @ point_lidar\n",
    "            corners_cam[:, i] = point_cam[:3]\n",
    "        \n",
    "        # 카메라 좌표계에서 이미지 평면으로 투영\n",
    "        corners_img = np.zeros((2, 8))\n",
    "        for i in range(8):\n",
    "            point_cam = corners_cam[:, i]\n",
    "            point_img = camera_to_image(np.array([point_cam]), calib_data)[0]\n",
    "            corners_img[:, i] = point_img\n",
    "        \n",
    "        # 이미지 평면 내 코너만 필터링\n",
    "        valid_corners = []\n",
    "        for i in range(8):\n",
    "            x, y = corners_img[:, i]\n",
    "            if 0 <= x < width and 0 <= y < height:\n",
    "                valid_corners.append((x, y))\n",
    "        \n",
    "        # 유효한 코너가 충분히 있는 경우 2D 박스 계산\n",
    "        if len(valid_corners) >= 2:\n",
    "            valid_corners = np.array(valid_corners)\n",
    "            min_x = np.min(valid_corners[:, 0])\n",
    "            min_y = np.min(valid_corners[:, 1])\n",
    "            max_x = np.max(valid_corners[:, 0])\n",
    "            max_y = np.max(valid_corners[:, 1])\n",
    "            \n",
    "            # 2D 박스가 너무 작지 않은 경우에만 추가\n",
    "            if max_x - min_x > 10 and max_y - min_y > 10:\n",
    "                det_copy = det.copy()\n",
    "                det_copy['2d_box'] = [min_x, min_y, max_x, max_y]\n",
    "                results.append(det_copy)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fuse_3d_2d_detections(detections_3d_projected, detections_2d, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    투영된 3D 검출과 2D 검출 결과를 융합\n",
    "    \n",
    "    Args:\n",
    "        detections_3d_projected (list): 이미지 평면에 투영된 3D 검출 결과\n",
    "        detections_2d (list): 2D 검출 결과\n",
    "        iou_threshold (float): 동일 객체로 판단하는 IoU 임계값\n",
    "        \n",
    "    Returns:\n",
    "        list: 융합된 검출 결과\n",
    "    \"\"\"\n",
    "    fused_detections = []\n",
    "    used_3d_indices = set()\n",
    "    used_2d_indices = set()\n",
    "    \n",
    "    # 각 3D 검출에 대해 가장 일치하는 2D 검출 찾기\n",
    "    for i, det_3d in enumerate(detections_3d_projected):\n",
    "        if '2d_box' not in det_3d:\n",
    "            continue\n",
    "            \n",
    "        box_3d = det_3d['2d_box']\n",
    "        best_iou = 0\n",
    "        best_match = -1\n",
    "        \n",
    "        for j, det_2d in enumerate(detections_2d):\n",
    "            box_2d = det_2d['box']\n",
    "            iou = compute_iou(box_3d, box_2d)\n",
    "            \n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match = j\n",
    "        \n",
    "        # 충분히 일치하는 2D 검출이 있는 경우 융합\n",
    "        if best_iou > iou_threshold:\n",
    "            det_2d = detections_2d[best_match]\n",
    "            \n",
    "            # 3D 정보는 3D 검출에서, 클래스 신뢰도는 가중 평균 사용\n",
    "            fused_det = det_3d.copy()\n",
    "            \n",
    "            # 클래스가 다른 경우 신뢰도 비교\n",
    "            if det_3d['class'] != det_2d['class']:\n",
    "                if det_3d['confidence'] > det_2d['confidence']:\n",
    "                    fused_class = det_3d['class']\n",
    "                else:\n",
    "                    fused_class = det_2d['class']\n",
    "                    \n",
    "                fused_det['class'] = fused_class\n",
    "            \n",
    "            # 신뢰도는 가중 평균으로 갱신\n",
    "            fused_det['confidence'] = 0.7 * det_3d['confidence'] + 0.3 * det_2d['confidence']\n",
    "            \n",
    "            # 2D 경계 상자 위치 정보 반영 (IoU에 따라 가중치 부여)\n",
    "            weight_3d = 0.7\n",
    "            weight_2d = 0.3\n",
    "            \n",
    "            # IoU가 높을수록 2D 가중치를 높임\n",
    "            if best_iou > 0.7:\n",
    "                weight_3d = 0.3\n",
    "                weight_2d = 0.7\n",
    "            \n",
    "            fused_det['2d_box'] = [\n",
    "                weight_3d * box_3d[0] + weight_2d * det_2d['box'][0],\n",
    "                weight_3d * box_3d[1] + weight_2d * det_2d['box'][1],\n",
    "                weight_3d * box_3d[2] + weight_2d * det_2d['box'][2],\n",
    "                weight_3d * box_3d[3] + weight_2d * det_2d['box'][3]\n",
    "            ]\n",
    "            \n",
    "            fused_det['source'] = 'both'\n",
    "            fused_det['3d_confidence'] = det_3d['confidence']\n",
    "            fused_det['2d_confidence'] = det_2d['confidence']\n",
    "            \n",
    "            fused_detections.append(fused_det)\n",
    "            \n",
    "            used_3d_indices.add(i)\n",
    "            used_2d_indices.add(best_match)\n",
    "        \n",
    "    # 매칭되지 않은 3D 검출 추가\n",
    "    for i, det_3d in enumerate(detections_3d_projected):\n",
    "        if i not in used_3d_indices and '2d_box' in det_3d:\n",
    "            det_copy = det_3d.copy()\n",
    "            det_copy['source'] = '3d'\n",
    "            det_copy['3d_confidence'] = det_3d['confidence']\n",
    "            det_copy['2d_confidence'] = 0.0\n",
    "            fused_detections.append(det_copy)\n",
    "    \n",
    "    # 매칭되지 않은 2D 검출 추가\n",
    "    for i, det_2d in enumerate(detections_2d):\n",
    "        if i not in used_2d_indices:\n",
    "            det_copy = {\n",
    "                'id': len(detections_3d_projected) + i,\n",
    "                'class': det_2d['class'],\n",
    "                'confidence': det_2d['confidence'],\n",
    "                '2d_box': det_2d['box'],\n",
    "                'source': '2d',\n",
    "                '3d_confidence': 0.0,\n",
    "                '2d_confidence': det_2d['confidence']\n",
    "            }\n",
    "            fused_detections.append(det_copy)\n",
    "    \n",
    "    return fused_detections\n",
    "\n",
    "# 3D 검출 결과 시뮬레이션\n",
    "detections_3d = simulate_3d_detections(num_objects=6)\n",
    "\n",
    "# 3D 검출 결과를 이미지 평면에 투영\n",
    "detections_3d_projected = project_3d_to_2d(detections_3d, calib_data, image.shape[:2])\n",
    "\n",
    "# 2D 검출 결과 시뮬레이션 (일부는 3D 검출과 일치, 일부는 새로운 검출)\n",
    "camera_detections, _ = simulate_object_detection(num_objects=8, seed=43)\n",
    "\n",
    "# 3D-2D 융합\n",
    "fused_3d_2d_detections = fuse_3d_2d_detections(detections_3d_projected, camera_detections, iou_threshold=0.3)\n",
    "\n",
    "print(f\"3D 검출 수: {len(detections_3d)}\")\n",
    "print(f\"이미지에 투영된 3D 검출 수: {len(detections_3d_projected)}\")\n",
    "print(f\"2D 검출 수: {len(camera_detections)}\")\n",
    "print(f\"융합된 검출 수: {len(fused_3d_2d_detections)}\")\n",
    "\n",
    "# 3D-2D 융합 결과 시각화\n",
    "def visualize_3d_2d_fusion(image, detections_3d_projected, detections_2d, fused_detections):\n",
    "    \"\"\"3D-2D 융합 결과 시각화\"\"\"\n",
    "    # 이미지 복사\n",
    "    img_3d = image.copy()\n",
    "    img_2d = image.copy()\n",
    "    img_fused = image.copy()\n",
    "    \n",
    "    # 3D 검출 시각화\n",
    "    for det in detections_3d_projected:\n",
    "        if '2d_box' in det:\n",
    "            box = det['2d_box']\n",
    "            box = [int(x) for x in box]\n",
    "            cv2.rectangle(img_3d, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
    "            label = f\"{det['class']} {det['confidence']:.2f}\"\n",
    "            cv2.putText(img_3d, label, (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    # 2D 검출 시각화\n",
    "    for det in detections_2d:\n",
    "        box = det['box']\n",
    "        box = [int(x) for x in box]\n",
    "        cv2.rectangle(img_2d, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        label = f\"{det['class']} {det['confidence']:.2f}\"\n",
    "        cv2.putText(img_2d, label, (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # 융합 검출 시각화\n",
    "    for det in fused_detections:\n",
    "        if '2d_box' in det:\n",
    "            box = det['2d_box']\n",
    "            box = [int(x) for x in box]\n",
    "            \n",
    "            # 소스에 따라 색상 설정\n",
    "            if det['source'] == 'both':\n",
    "                color = (255, 0, 255)  # 마젠타\n",
    "            elif det['source'] == '3d':\n",
    "                color = (255, 0, 0)    # 빨강\n",
    "            else:  # '2d'\n",
    "                color = (0, 255, 0)    # 초록\n",
    "                \n",
    "            cv2.rectangle(img_fused, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "            label = f\"{det['class']} {det['confidence']:.2f}\"\n",
    "            cv2.putText(img_fused, label, (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    # 결과 표시\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(img_3d)\n",
    "    axes[0].set_title('3D Detections (Projected)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img_2d)\n",
    "    axes[1].set_title('2D Detections')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(img_fused)\n",
    "    axes[2].set_title('Fused 3D-2D Detections')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3D-2D 융합 결과 시각화\n",
    "visualize_3d_2d_fusion(image, detections_3d_projected, camera_detections, fused_3d_2d_detections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 센서 융합의 비교 및 평가\n",
    "\n",
    "다양한 센서 융합 방식의 장단점을 비교하고 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 센서 융합 방식 비교 및 평가 표\n",
    "fusion_comparison = {\n",
    "    'Early Fusion': {\n",
    "        'pros': [\n",
    "            '원시 데이터 수준의 완전한 정보 활용 가능',\n",
    "            '센서 간 상호 보완적 특성 직접 활용',\n",
    "            '단일 처리 파이프라인으로 구현 간단',\n",
    "            '각 센서의 약점 직접 보완 가능'\n",
    "        ],\n",
    "        'cons': [\n",
    "            '센서 동기화 필수',\n",
    "            '서로 다른 특성의 데이터 결합 어려움',\n",
    "            '처리해야 할 데이터 양이 많음',\n",
    "            '한 센서의 노이즈가 전체 성능에 영향'\n",
    "        ],\n",
    "        'examples': [\n",
    "            'RGB-D 데이터 생성',\n",
    "            '포인트 클라우드 컬러링',\n",
    "            '다중 센서 포인트 클라우드 병합'\n",
    "        ]\n",
    "    },\n",
    "    'Mid Fusion': {\n",
    "        'pros': [\n",
    "            '각 센서의 고유한 특징 추출 활용',\n",
    "            '다양한 특성의 데이터 융합 가능',\n",
    "            '특징 수준에서 노이즈 필터링 가능',\n",
    "            '각 센서의 장점을 결합한 특징 생성'\n",
    "        ],\n",
    "        'cons': [\n",
    "            '특징 추출 방법에 따라 성능 변동',\n",
    "            '특징 간 상관관계 모델링 복잡',\n",
    "            '최적 특징 선택의 어려움',\n",
    "            '각 센서별 특징 추출기 필요'\n",
    "        ],\n",
    "        'examples': [\n",
    "            '딥러닝 기반 특징 융합',\n",
    "            '멀티모달 특징 학습',\n",
    "            'LiDAR-카메라 특징 융합 객체 검출'\n",
    "        ]\n",
    "    },\n",
    "    'Late Fusion': {\n",
    "        'pros': [\n",
    "            '각 센서별 독립적인 처리 가능',\n",
    "            '센서 추가/제거 용이',\n",
    "            '센서 고장에 강인함',\n",
    "            '기존 알고리즘 재활용 가능'\n",
    "        ],\n",
    "        'cons': [\n",
    "            '저수준 정보 활용 어려움',\n",
    "            '중복 계산 발생',\n",
    "            '각 센서의 약점을 근본적으로 극복하기 어려움',\n",
    "            '결정 융합 규칙 설계의 어려움'\n",
    "        ],\n",
    "        'examples': [\n",
    "            '다중 센서 객체 검출 결과 융합',\n",
    "            '레이더-카메라 트래킹 결과 융합',\n",
    "            '다양한 클래스 분류기 앙상블'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 표 형태로 시각화\n",
    "def visualize_fusion_comparison(comparison_data):\n",
    "    \"\"\"센서 융합 방식 비교 표 시각화\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 표 헤더\n",
    "    headers = ['Fusion Type', 'Pros', 'Cons', 'Examples']\n",
    "    \n",
    "    # 표 데이터 준비\n",
    "    table_data = []\n",
    "    for fusion_type, data in comparison_data.items():\n",
    "        pros = '\\n'.join([f\"• {item}\" for item in data['pros']])\n",
    "        cons = '\\n'.join([f\"• {item}\" for item in data['cons']])\n",
    "        examples = '\\n'.join([f\"• {item}\" for item in data['examples']])\n",
    "        table_data.append([fusion_type, pros, cons, examples])\n",
    "    \n",
    "    # 표 생성\n",
    "    table = ax.table(cellText=table_data, colLabels=headers, loc='center', cellLoc='center')\n",
    "    \n",
    "    # 표 스타일 설정\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # 각 셀 너비 조정\n",
    "    for i, width in enumerate([0.15, 0.3, 0.3, 0.25]):\n",
    "        for j in range(len(table_data) + 1):\n",
    "            cell = table[(j, i)]\n",
    "            cell.set_width(width)\n",
    "            \n",
    "            # 헤더 셀 색상 설정\n",
    "            if j == 0:\n",
    "                cell.set_facecolor('#4472C4')\n",
    "                cell.set_text_props(color='white')\n",
    "            else:\n",
    "                # 행 별 색상 번갈아 설정\n",
    "                if j % 2 == 0:\n",
    "                    cell.set_facecolor('#D9E1F2')\n",
    "                else:\n",
    "                    cell.set_facecolor('#E9EDF4')\n",
    "    \n",
    "    plt.title('Comparison of Sensor Fusion Approaches', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fusion_comparison.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# 센서 융합 방식 비교 표 시각화\n",
    "visualize_fusion_comparison(fusion_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 요약 및 결론\n",
    "\n",
    "이 노트북에서는 자율주행을 위한 다양한 센서 융합 기법을 살펴보고 구현해보았습니다.\n",
    "\n",
    "주요 학습 내용:\n",
    "\n",
    "1. 센서 융합의 기본 개념과 필요성\n",
    "2. 초기/중기/후기 융합 방식의 차이점과 특징\n",
    "3. RGB-D 데이터 생성을 통한 초기 융합 구현\n",
    "4. 특징 추출 및 융합을 통한 중기 융합 구현\n",
    "5. 객체 검출 결과 융합을 통한 후기 융합 구현\n",
    "6. 3D-2D 융합 파이프라인 구현\n",
    "7. 다양한 센서 융합 방식의 장단점 비교 및 평가\n",
    "\n",
    "센서 융합은 자율주행 시스템의 인지 성능을 향상시키는 핵심 기술입니다. 각 센서의 장점을 결합하고 단점을 완하여 더 정확하고 신뢰성 있는 환경 인식을 가능하게 합니다.\n",
    "\n",
    "## 다음 단계\n",
    "\n",
    "- 시간적 센서 융합 구현 (여러 프레임에 걸친 센서 데이터 융합)\n",
    "- 딥러닝 기반 멀티모달 융합 아키텍처 연구\n",
    "- 불확실성을 고려한 센서 융합 방법 연구\n",
    "- 다양한 조건(날씨, 조명 등)에서의 센서 융합 성능 평가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar-perception",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
